{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import parse, split\n",
    "import codecs\n",
    "import nltk \n",
    "nltk.download('wordnet') \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "wnl = WordNetLemmatizer() \n",
    "\n",
    "file=codecs.open('englishTest.txt', 'r')\n",
    "\n",
    "newList = nltk.word_tokenize(file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/etnaramirez/Documents/8 semestre/IC/ProyectoAA/procesamiento.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/etnaramirez/Documents/8%20semestre/IC/ProyectoAA/procesamiento.ipynb#ch0000001?line=4'>5</a>\u001b[0m data_idx\u001b[39m=\u001b[39mdata_idx\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/etnaramirez/Documents/8%20semestre/IC/ProyectoAA/procesamiento.ipynb#ch0000001?line=5'>6</a>\u001b[0m data_idx\u001b[39m=\u001b[39mdata_idx\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/etnaramirez/Documents/8%20semestre/IC/ProyectoAA/procesamiento.ipynb#ch0000001?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/etnaramirez/Documents/8%20semestre/IC/ProyectoAA/procesamiento.ipynb#ch0000001?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/etnaramirez/Documents/8%20semestre/IC/ProyectoAA/procesamiento.ipynb#ch0000001?line=11'>12</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer() \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_idx= pd.read_csv(\"Gold-Ingles.csv\",  header=None)\n",
    "data_idx= data_idx.drop(columns=[1,2])\n",
    "data_idx=data_idx.to_numpy()\n",
    "data_idx=data_idx.flatten()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "file=open(\"sw_filter.txt\",encoding='utf-8', errors='ignore')\n",
    "lines=file.readlines()\n",
    "\n",
    "\n",
    "\n",
    "def Lematizando(sentence, idx):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "\n",
    "    for word in token_words:\n",
    "        if(word in idx):\n",
    "            stem_sentence.append(word)\n",
    "            stem_sentence.append(\" \")\n",
    "        else:\n",
    "            stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "            stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "for line in lines:    \n",
    "    stem_sentence=Lematizando(line, data_idx)\n",
    "    newFile = open('lematized.txt','a') \n",
    "    newFile.write(stem_sentence+\"\\n\")\n",
    "\n",
    "newFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file=open(\"English.txt\", encoding='utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documentA = 'the man went out for a walk'\n",
    "documentB = 'the children sat around the fire'\n",
    "\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "\n",
    "\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cacbb3c092fab53a60e9a71562ab76ab5ec75d7d3bd960d0f30450a9f52945e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('IC': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
